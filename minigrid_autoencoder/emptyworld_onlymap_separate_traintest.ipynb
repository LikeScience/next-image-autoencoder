{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9788dd5e",
   "metadata": {},
   "source": [
    "#### Imports and env creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96a09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import annotations\n",
    "\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.constants import IDX_TO_OBJECT, OBJECT_TO_IDX\n",
    "\n",
    "from minigrid.core.actions import Actions\n",
    "from dataclasses import dataclass, field \n",
    "from typing import Set\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c267bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  2  2  2  2  2]\n",
      " [ 2 10  1  2  1  1  2]\n",
      " [ 2  1  1  2  1  1  2]\n",
      " [ 2  1  1  1  1  1  2]\n",
      " [ 2  1  1  2  1  1  2]\n",
      " [ 2  1  1  2  1  8  2]\n",
      " [ 2  2  2  2  2  2  2]]\n"
     ]
    }
   ],
   "source": [
    "WORLD_N=7\n",
    "WORLD_N_SQ = WORLD_N**2\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=WORLD_N,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=256,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"grand mission\"\n",
    "    # MiniGridEnv._gen_grid\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "      self.grid = Grid(width, height)\n",
    "      self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "      if self.agent_start_pos is not None:\n",
    "          self.agent_pos = self.agent_start_pos\n",
    "          self.agent_dir = self.agent_start_dir\n",
    "      else:\n",
    "          self.place_agent()\n",
    "      self.valid_actions = {Actions.left, Actions.right, Actions.forward}\n",
    "    \n",
    "      \n",
    "      self.put_obj(Goal(), width - 2, height - 2)\n",
    "      \n",
    "      self.grid.set(3, 1, Wall())\n",
    "      self.grid.set(3, 2, Wall())\n",
    "      self.grid.set(3, 4, Wall())\n",
    "      self.grid.set(3, 5, Wall())\n",
    "    #   for i in range(0, height):\n",
    "    #     self.grid.set(5, i, Wall())\n",
    "\n",
    "    #   self.grid.set(5, 6, Door(COLOR_NAMES[0], is_locked=True))\n",
    "    #   self.grid.set(3, 6, Key(COLOR_NAMES[0]))\n",
    "    \n",
    "    def get_array_repr(self, with_agent=True):\n",
    "        grid_array = self.unwrapped.grid.encode()[:,:,0]\n",
    "        # print(grid_array)\n",
    "        # print(self.agent_pos)\n",
    "        grid_array[self.agent_pos[0],self.agent_pos[1]]=OBJECT_TO_IDX['agent']\n",
    "        return grid_array.T\n",
    "    \n",
    "\n",
    "# env = SimpleEnv(render_mode=\"human\")\n",
    "# manual_control = ManualControl(env, seed=42)\n",
    "\n",
    "env = SimpleEnv()\n",
    "env.reset()\n",
    "print(env.get_array_repr())\n",
    "\n",
    "# enable manual control for testing\n",
    "# manual_control.start()\n",
    "\n",
    "map_numbers = [1,2,8,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c468843",
   "metadata": {},
   "source": [
    "#### Agent motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83bc3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  1,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1]), array([ 2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  2,  1,  1,  2,  2, 10,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1])]\n"
     ]
    }
   ],
   "source": [
    "from agents.random import RandomAgent\n",
    "n_steps_train = 8000\n",
    "n_steps_test = 2000\n",
    "\n",
    "random_action_agent = RandomAgent(valid_actions=env.valid_actions)\n",
    "\n",
    "dirarr = ['Right', 'Down', 'Left','Up']\n",
    "actions_to_idx = {Actions.left:0, Actions.right:1, Actions.forward:2}\n",
    "\n",
    "image_list_train = []\n",
    "\n",
    "for i in range(n_steps_train):\n",
    "    action = random_action_agent.act()\n",
    "    arr = env.get_array_repr().flatten()\n",
    "    arr = np.append(arr, [actions_to_idx[action], env.agent_dir])\n",
    "    image_list_train.append(arr)\n",
    "    # print(dirarr[env.agent_dir])\n",
    "    # print(action)\n",
    "    env.step(action)\n",
    "    # print(env.get_array_repr())\n",
    "\n",
    "env.reset()\n",
    "image_list_test = []\n",
    "\n",
    "for i in range(n_steps_test):\n",
    "    action = random_action_agent.act()\n",
    "    arr = env.get_array_repr().flatten()\n",
    "    arr = np.append(arr, [actions_to_idx[action], env.agent_dir])\n",
    "    image_list_test.append(arr)\n",
    "    # print(dirarr[env.agent_dir])\n",
    "    # print(action)\n",
    "    env.step(action)\n",
    "\n",
    "print(image_list_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4188951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  1,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1]), array([ 2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  2,  1,  1,  2,  2, 10,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1])]\n",
      "[array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  0,  3]), array([ 2,  2,  2,  2,  2,  2,  2,  2, 10,  1,  2,  1,  1,  2,  2,  1,  1,\n",
      "        2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,\n",
      "        2,  2,  1,  1,  2,  1,  8,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2])]\n"
     ]
    }
   ],
   "source": [
    "print(image_list_train[:3])\n",
    "print(image_list_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f2565",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fb66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db192e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n",
      "True\n",
      "0\n",
      "NVIDIA GeForce MX330\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # This should return True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Shows the current GPU device id (e.g., 0)\n",
    "print(torch.cuda.get_device_name(0))  # Displays the GPU name\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd3d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Modified) Neuromatch helper funcitons\n",
    "def init_weights_kaiming_normal(layer):\n",
    "  \"\"\"\n",
    "  Initializes weights from linear PyTorch layer\n",
    "  with kaiming normal distribution.\n",
    "\n",
    "  Args:\n",
    "    layer (torch.Module)\n",
    "        Pytorch layer\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  # check for linear PyTorch layer\n",
    "  if isinstance(layer, nn.Linear):\n",
    "    # initialize weights with kaiming normal distribution\n",
    "    nn.init.kaiming_normal_(layer.weight.data)\n",
    "    \n",
    "\n",
    "def runSGD(net, input_train, target_train, input_test, target_test, criterion='mse',\n",
    "           n_epochs=10, batch_size=32, verbose=False):\n",
    "  \"\"\"\n",
    "  Trains autoencoder network with stochastic gradient descent with Adam\n",
    "  optimizer and loss criterion. Train samples are shuffled, and loss is\n",
    "  displayed at the end of each opoch for both MSE and BCE. Plots training loss\n",
    "  at each minibatch (maximum of 500 randomly selected values).\n",
    "\n",
    "  Args:\n",
    "    net (torch network)\n",
    "        ANN object (nn.Module)\n",
    "\n",
    "    input_train (torch.Tensor)\n",
    "        vectorized input images from train set\n",
    "\n",
    "    input_test (torch.Tensor)\n",
    "        vectorized input images from test set\n",
    "\n",
    "    criterion (string)\n",
    "        train loss: 'bce' or 'mse'\n",
    "\n",
    "    n_epochs (boolean)\n",
    "        number of full iterations of training data\n",
    "\n",
    "    batch_size (integer)\n",
    "        number of element in mini-batches\n",
    "\n",
    "    verbose (boolean)\n",
    "        print final loss\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Define the device\n",
    "  # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  device = torch.device('cpu')\n",
    "  print(f\"Using device: {device}\")\n",
    "\n",
    "  # 2. Move the network to the device\n",
    "  net.to(device)\n",
    "\n",
    "  # 3. Move the main tensors to the device (crucial for initial setup)\n",
    "  input_train = input_train.to(device)\n",
    "  target_train = target_train.to(device)\n",
    "  input_test = input_test.to(device)\n",
    "  target_test = target_test.to(device)\n",
    "\n",
    "  # Initialize loss function\n",
    "  if criterion == 'mse':\n",
    "    loss_fn = nn.MSELoss()\n",
    "  elif criterion == 'bce':\n",
    "    loss_fn = nn.BCELoss()\n",
    "  elif criterion == 'cel':\n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "  else:\n",
    "    print('Please specify either \"mse\" or \"bce\" for loss criterion')\n",
    "\n",
    "  # Move the loss function to the device if it has parameters (CrossEntropyLoss does not, \n",
    "  # but it's good practice for others like L1Loss which might have reduction='none')\n",
    "  loss_fn.to(device)\n",
    "\n",
    "  # Initialize SGD optimizer\n",
    "  optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "  # Placeholder for loss\n",
    "  track_loss = []\n",
    "\n",
    "  print('Epoch', '\\t', 'Loss train', '\\t', 'Loss test')\n",
    "  for i in range(n_epochs):\n",
    "\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(len(input_train))\n",
    "\n",
    "    batches_input = torch.split(input_train[shuffle_idx], batch_size)\n",
    "    batches_target = torch.split(target_train[shuffle_idx], batch_size)\n",
    "\n",
    "    batches = zip(batches_input, batches_target)\n",
    "\n",
    "\n",
    "    shuffle_idx = np.random.permutation(len(input_train))\n",
    "    # batches = torch.split(input_train[shuffle_idx], batch_size)\n",
    "    # for batch in batches:\n",
    "    #   output_train = net(batch)\n",
    "    #   loss = loss_fn(output_train, batch)\n",
    "    for batch_input, batch_target in batches:\n",
    "      batch_input = batch_input.float()\n",
    "      batch_target = batch_target.float()\n",
    "      output_train = net(batch_input)  # Forward pass on the input batch\n",
    "      loss = loss_fn(output_train, batch_target)  # Compare output with the target\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Keep track of loss at each epoch\n",
    "      track_loss += [float(loss)]\n",
    "\n",
    "    loss_epoch = f'{i+1}/{n_epochs}'\n",
    "    with torch.no_grad():\n",
    "      output_train = net(input_train)\n",
    "      loss_train = loss_fn(output_train, target_train)\n",
    "      loss_epoch += f'\\t {loss_train:.4f}'\n",
    "\n",
    "      output_test = net(input_test)\n",
    "      loss_test = loss_fn(output_test, target_test)\n",
    "      loss_epoch += f'\\t\\t {loss_test:.4f}'\n",
    "\n",
    "    print(loss_epoch)\n",
    "\n",
    "  if verbose:\n",
    "    # Print loss\n",
    "    loss_mse = f'\\nMSE\\t {eval_mse(output_train, target_train):0.4f}'\n",
    "    loss_mse += f'\\t\\t {eval_mse(output_test, target_test):0.4f}'\n",
    "    print(loss_mse)\n",
    "\n",
    "    loss_bce = f'BCE\\t {eval_bce(output_train, target_train):0.4f}'\n",
    "    loss_bce += f'\\t\\t {eval_bce(output_test, target_test):0.4f}'\n",
    "    print(loss_bce)\n",
    "\n",
    "  # Plot loss\n",
    "  step = int(np.ceil(len(track_loss) / 500))\n",
    "  x_range = np.arange(0, len(track_loss), step)\n",
    "  plt.figure()\n",
    "  plt.plot(x_range, track_loss[::step], 'C0')\n",
    "  plt.xlabel('Iterations')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlim([0, None])\n",
    "  plt.ylim([0, None])\n",
    "  plt.show()\n",
    "  print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0d9a3",
   "metadata": {},
   "source": [
    "#### Input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f1276ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2,  ..., 2, 2, 1],\n",
      "        [2, 2, 2,  ..., 2, 2, 1],\n",
      "        [2, 2, 2,  ..., 2, 2, 1],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 2, 0],\n",
      "        [2, 2, 2,  ..., 2, 2, 0],\n",
      "        [2, 2, 2,  ..., 2, 2, 0]])\n",
      "tensor([[2, 2, 2,  ..., 2, 1, 0],\n",
      "        [2, 2, 2,  ..., 2, 2, 1],\n",
      "        [2, 2, 2,  ..., 2, 2, 1],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 0, 1],\n",
      "        [2, 2, 2,  ..., 2, 2, 0],\n",
      "        [2, 2, 2,  ..., 2, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "# test_size=0.2\n",
    "# cutoff = int((1-test_size)*len(image_list))\n",
    "X_train, X_test = image_list_train, image_list_test \n",
    "\n",
    "target_train = X_train[1:]\n",
    "input_train = X_train[:-1]\n",
    "target_test = X_test[1:]\n",
    "input_test = X_test[:-1]\n",
    "\n",
    "target_train = torch.tensor(target_train, dtype=torch.int64)\n",
    "input_train = torch.tensor(input_train, dtype=torch.int64)\n",
    "target_test = torch.tensor(target_test, dtype=torch.int64)\n",
    "input_test = torch.tensor(input_test, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def one_hot_encode_set(values, classes):\n",
    "    classdict = {}\n",
    "    for i in range(len(classes)):\n",
    "        classdict[classes[i]] = i\n",
    "    one_hot = torch.zeros(len(classes))\n",
    "    one_hot[classdict[values.item()]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def continuous_class(values, classes, direct):\n",
    "    classdict = {}\n",
    "    for i in range(len(classes)):\n",
    "        classdict[classes[i]] = i\n",
    "    val = classdict[values.item()]\n",
    "    if (values.item()==10):\n",
    "        val+=direct\n",
    "    return torch.tensor([val])\n",
    "\n",
    "\n",
    "# Define a function for one-hot encoding\n",
    "def one_hot_encode(values, num_classes):\n",
    "    one_hot = torch.zeros(num_classes)\n",
    "    one_hot[values] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Function to process the entire array (each input array of shape [length])\n",
    "def process_input(input_array, mode):\n",
    "    if mode == 'in':\n",
    "    \n",
    "        # first_part = torch.cat([one_hot_encode(input_array[i], 11) for i in range(25)])\n",
    "        # first_part = torch.cat([one_hot_encode_set(input_array[i], map_numbers) for i in range(25)])\n",
    "        first_part = torch.cat([continuous_class(input_array[i], map_numbers, input_array[1+WORLD_N_SQ]) for i in range(WORLD_N_SQ)])\n",
    "\n",
    "        # 26th position: one-hot encoded into 3 classes\n",
    "        second_part = one_hot_encode(input_array[WORLD_N_SQ], 3)\n",
    "        \n",
    "        # Last position: one-hot encoded into 4 classes\n",
    "        # third_part = one_hot_encode(input_array[26], 4)\n",
    "\n",
    "        # print(torch.cat([first_part, second_part, third_part], dim=0))\n",
    "        # Concatenate all parts to form the final one-hot encoded array\n",
    "        return torch.cat([first_part, second_part], dim=0)\n",
    "    elif mode == 'out':\n",
    "        return torch.cat([continuous_class(input_array[i], map_numbers, input_array[1+WORLD_N_SQ]) for i in range(WORLD_N_SQ)])\n",
    "\n",
    "\n",
    "# Apply to the whole dataset\n",
    "input_orientation_train = [x[1+WORLD_N_SQ] for x in input_train]\n",
    "input_orientation_test = [x[1+WORLD_N_SQ] for x in input_test]\n",
    "target_orientation_train = [x[1+WORLD_N_SQ] for x in target_train]\n",
    "target_orientation_test = [x[1+WORLD_N_SQ] for x in target_test]\n",
    "\n",
    "# Apply to the whole dataset\n",
    "input_train_processed = torch.stack([process_input(x, 'in') for x in input_train])\n",
    "input_test_processed = torch.stack([process_input(x, 'in') for x in input_test])\n",
    "target_train_processed = torch.stack([process_input(x, 'out') for x in target_train])\n",
    "target_test_processed = torch.stack([process_input(x, 'out') for x in target_test])\n",
    "\n",
    "\n",
    "print(target_train)\n",
    "print(input_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b84df5",
   "metadata": {},
   "source": [
    "#### Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "43198093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder \n",
      "\n",
      " Sequential(\n",
      "  (0): Linear(in_features=52, out_features=780, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=780, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "Decoder \n",
      "\n",
      " Sequential(\n",
      "  (4): Linear(in_features=50, out_features=735, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=735, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoding_size=50\n",
    "input_size=input_train_processed.size(1)\n",
    "output_size = target_train_processed.size(1)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, int(input_size * 15)),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(int(input_size * 15), encoding_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(encoding_size, int(output_size * 15)),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(int(output_size *15), output_size),\n",
    "    )\n",
    "\n",
    "# model[:-2].apply(init_weights_kaiming_normal)\n",
    "\n",
    "n_l = 4\n",
    "encoder = model[:n_l]\n",
    "decoder = model[n_l:]\n",
    "print(f'Encoder \\n\\n {encoder}\\n')\n",
    "print(f'Decoder \\n\\n {decoder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c672a46",
   "metadata": {},
   "source": [
    "#### Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "19c23a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch \t Loss train \t Loss test\n",
      "1/20\t 0.1160\t\t 0.1128\n",
      "2/20\t 0.0917\t\t 0.0899\n",
      "3/20\t 0.0717\t\t 0.0694\n",
      "4/20\t 0.0512\t\t 0.0522\n",
      "5/20\t 0.0264\t\t 0.0286\n",
      "6/20\t 0.0177\t\t 0.0196\n",
      "7/20\t 0.0271\t\t 0.0276\n",
      "8/20\t 0.0093\t\t 0.0106\n",
      "9/20\t 0.0038\t\t 0.0045\n",
      "10/20\t 0.0052\t\t 0.0071\n",
      "11/20\t 0.0019\t\t 0.0023\n",
      "12/20\t 0.0008\t\t 0.0010\n",
      "13/20\t 0.0021\t\t 0.0023\n",
      "14/20\t 0.0110\t\t 0.0104\n",
      "15/20\t 0.0004\t\t 0.0004\n",
      "16/20\t 0.0002\t\t 0.0002\n",
      "17/20\t 0.0011\t\t 0.0011\n",
      "18/20\t 0.0113\t\t 0.0113\n",
      "19/20\t 0.0004\t\t 0.0004\n",
      "20/20\t 0.0001\t\t 0.0002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "runSGD(model, input_train_processed, target_train_processed, input_test_processed, target_test_processed, n_epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c6af472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2,  ..., 2, 2, 3],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 3],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 2, 3],\n",
      "        [2, 2, 2,  ..., 2, 2, 3],\n",
      "        [2, 2, 2,  ..., 2, 2, 0]])\n",
      "Cell accuracy: 1.0\n",
      "Agent orientation accuracy: 1.0\n",
      "Number of agents accuracy: 1.0\n",
      "Agent location accuracy: 1.0\n",
      "Perfectness: 1.0\n"
     ]
    }
   ],
   "source": [
    "input_test_processed = input_test_processed.to(device)\n",
    "output = model(input_test_processed)  # Model output (logits or probabilities)\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# print(output)\n",
    "output = output.cpu() \n",
    "# Initialize a list to hold the decoded outputs for each observation\n",
    "decoded_outputs = []\n",
    "\n",
    "incorrect_agent_num=0\n",
    "incorrect_agent_location=0\n",
    "incorrect_orientation=0\n",
    "incorrect_cell=0\n",
    "not_perfect=0\n",
    "\n",
    "for i in range(output.size(0)):  # Loop over each sample in the batch\n",
    "    # Extract the blocks of the output\n",
    "    fist_NSQ = output[i]\n",
    "    # second_block = output[i, input_train_processed.size(1)-7:input_train_processed.size(1)-4]    # Next block of size 3\n",
    "    # third_block = output[i, input_train_processed.size(1)-7:]             # Last block of size 4\n",
    "\n",
    "    # Decode each block by taking the argmax (index of max value)\n",
    "    decoded_fist_NSQ = torch.round(fist_NSQ).long()  # 25 values, each between 0-10\n",
    "    # decoded_second = torch.argmax(second_block)       # Single value between 0-2\n",
    "    # decoded_third = torch.argmax(third_block)         # Single value between 0-3\n",
    "    \n",
    "    map_array = map_numbers+[11,12,13]\n",
    "    in_map_array = torch.isin(decoded_fist_NSQ, torch.tensor(range(len(map_array))))\n",
    "    # Convert values not in map_array to len(map_array)\n",
    "    adjusted_values = torch.where(in_map_array, decoded_fist_NSQ, torch.tensor(len(map_array)))\n",
    "    mapping = torch.tensor(map_array+[-1])\n",
    "    decoded_fist_NSQ = mapping[adjusted_values]\n",
    "\n",
    "    num_agents = 0\n",
    "    maybe_perfect=1\n",
    "\n",
    "    for j, val in enumerate(decoded_fist_NSQ):\n",
    "        if val >= 10 and num_agents == 0:\n",
    "            decoded_second = val -10\n",
    "            if (target_test[i, j] != 10):\n",
    "                incorrect_agent_location+=1\n",
    "                maybe_perfect=0\n",
    "        if val>= 10:\n",
    "            num_agents +=1\n",
    "        if (target_test[i, j] != val and (val < 10 or target_test[i, j] < 10)):\n",
    "            # print(\"cell\", i)\n",
    "            maybe_perfect=0\n",
    "            incorrect_cell +=1\n",
    "    \n",
    "    decoded_fist_NSQ[decoded_fist_NSQ > 10] = 10\n",
    "\n",
    "    if num_agents != 1:\n",
    "        decoded_second = torch.tensor(-1, dtype=torch.long)\n",
    "        maybe_perfect=0\n",
    "        incorrect_agent_num +=1\n",
    "    \n",
    "    if (decoded_second != target_orientation_test[i]):\n",
    "        maybe_perfect=0\n",
    "        incorrect_orientation +=1\n",
    "\n",
    "    if not maybe_perfect:\n",
    "        not_perfect +=1\n",
    "\n",
    "    # Use torch.masked_select to get all matching values\n",
    "    matches = torch.masked_select(decoded_fist_NSQ, decoded_fist_NSQ >= 10)\n",
    "\n",
    "    # Ensure it's a 1D tensor for concatenation\n",
    "    decoded_second = decoded_second.unsqueeze(0)\n",
    "\n",
    "    # Combine the decoded values into a single list for this observation\n",
    "    decoded_sample = torch.cat([decoded_fist_NSQ, decoded_second])\n",
    "    \n",
    "    # Append the decoded sample to the final list\n",
    "    decoded_outputs.append(decoded_sample)\n",
    "\n",
    "acc_cell = 1.0-(incorrect_cell/(WORLD_N_SQ*output.size(0)))\n",
    "acc_orientation = 1.0-(incorrect_orientation/output.size(0))\n",
    "acc_agentloc = 1.0-(incorrect_agent_location/output.size(0))\n",
    "acc_agentnum = 1.0-(incorrect_agent_num/output.size(0))\n",
    "acc_perfect = 1.0-(not_perfect/output.size(0))\n",
    "\n",
    "# Convert to tensor if needed\n",
    "decoded_outputs = torch.stack(decoded_outputs)\n",
    "print(decoded_outputs)\n",
    "\n",
    "print (\"Cell accuracy:\", acc_cell)\n",
    "print (\"Agent orientation accuracy:\", acc_orientation)\n",
    "print (\"Number of agents accuracy:\", acc_agentnum)\n",
    "print (\"Agent location accuracy:\", acc_agentloc)\n",
    "print (\"Perfectness:\", acc_perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e7485691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1301:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1, 10,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1, 10,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1, 10,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1302:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1, 10,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1303:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1304:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1305:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 3 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1306:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 2 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1307:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1308:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1309:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1310:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1311:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1, 10,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1312:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1313:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1314:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  1,  1,  2,  1,  8,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1315:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 2 | Target Orientation: 2 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1316:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1317:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1318:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1319:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1320:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 3 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1321:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 2 | Target Orientation: 2 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1322:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1323:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1324:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 2 | Target Orientation: 2 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1325:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 2\n",
      "==================================================\n",
      "Sample 1326:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 1\n",
      "==================================================\n",
      "Sample 1327:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 0\n",
      "Decoded Orientation: 3 | Target Orientation: 3 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1328:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 3\n",
      "==================================================\n",
      "Sample 1329:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 2\n",
      "Decoded Orientation: 0 | Target Orientation: 0 | Input Orientation: 0\n",
      "==================================================\n",
      "Sample 1330:\n",
      "Input Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Decoded Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Target Grid:\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  1,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1,  1,  2],\n",
      "        [ 2,  1,  1,  2,  1, 10,  2],\n",
      "        [ 2,  2,  2,  2,  2,  2,  2]], dtype=torch.int32)\n",
      "Input Action: 1\n",
      "Decoded Orientation: 1 | Target Orientation: 1 | Input Orientation: 0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "interval_min=1300\n",
    "interval_size=30\n",
    "for i in range(interval_min,interval_min+min(interval_size,decoded_outputs.size(0))):\n",
    "    # Print the input, decoded grid, and target test grid side by side\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    \n",
    "    # Input grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Input Grid:\")\n",
    "    print(input_test[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Decoded grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Decoded Grid:\")\n",
    "    print(decoded_outputs[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Target test grid (first 25 values reshaped into 5x5)\n",
    "    print(\"Target Grid:\")\n",
    "    print(target_test[i, :WORLD_N_SQ].reshape(WORLD_N, WORLD_N).int())\n",
    "    \n",
    "    # Action and Orientation for input, decoded, and target test\n",
    "    print(f\"Input Action: {input_test[i, WORLD_N_SQ].item()}\")\n",
    "    print(f\"Decoded Orientation: {decoded_outputs[i, WORLD_N_SQ].item()} | Target Orientation: {target_orientation_test[i].item()} | Input Orientation: {input_orientation_test[i].item()}\")\n",
    "    \n",
    "    print(\"=\" * 50)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f2204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg10 (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
